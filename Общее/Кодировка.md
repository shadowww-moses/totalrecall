Есть таблица символов, где каждому символу соответствует числовой код, а есть кодировка - уже непосредственно способ кодирования символов из этой таблицы. То есть таблица и кодировка - это не одно и то же.

Например, ASCII - это название и таблицы, и кодировки. Windows-1251 - тоже название и таблицы, и кодировки.

А вот Unicode - это название таблицы, а кодировки для нее есть разные - UTF-8, UTF-16, UTF-32

# ASCII

В этой кодировке все символы кодируются одним байтом. Семь первых бит используются для кодирования латинского алфавита, а также управляющих символов и псевдографики. А старший бит оставлен про запас, например для национальных символов.

Так что на этой кодировке успешно основывается например Windows-1251, которая благодаря этому старшему биту дополняет основные символы символами всех славянских языков.

Один байт позволяет закодировать 2^8 = 256 символов, так что этого не хватает, чтобы закодировать все языки. Поэтому придумали Unicode.

# UNICODE и UTF-%N

В таблицу Unicode помещено огромное количество символов, наверное все которые смогли вспомнить и придумать. Всего их получилось примерно миллиард (1 114 112). Максимальное количество - около двух миллиардов - 2^21 = 2 097 152

Почему именно 2^21?

Из-за такого большого размера таблицы одного байта не хватает, чтобы представить все коды символов, поэтому используется больше байтов, но часть бит отведена под служебные цели. Какие-такие служебные цели? Допустим, если использовать только латинские символы, тогда хватит и одного байта для каждого символа, потому что начало Unicode таблицы совпадает с таблицей ASCII. Но тратить в этом случае 4 байта на один символ, когда можно обойтись 1 байтом - неэффективно.

В таком случае нужно как-то различать, сколько байт использовано для кодирования символа, чтобы считать нужно количество байт и преобразовать в символ. Вот здесь и помогают служебные биты.

Если байт начинается с 0, тогда сразу понятно, что этот байт самостоятельный и представляет собой 1 символ. Если он начинается с 110, значит, символ представлен двумя байтами и чтобы получить символ, нужно считать подряд два байта. Если байт начинается с 1110, значит символ представлен тремя байтами, если с 11110 - четырьмя байтами.

Если байт начинается с 10, значит это какая-то часть символа, несамостоятельный байт. Таким образом, кодирующие-декодирующие программы могут распознавать ошибки. Если например, они получают байт 10ххх, но до этого не было какого-нибудь байта вроде 110ххх, то значит произошла ошибка и символ раскодировать не удастся.

Таким образом, получаем объяснение, почему именно 2^21 - с учетом того, что если в каждом "многобайтовом символе" в каждом байте максимум под служебку отводится 5 бит, а минимум - 2 бита, то для кодирования миллиарда символов потребуется минимум 4 байта. Это 32 бита, из которых получается под служебку максимум 11 бит - 11110ххх 10ххх 10ххх 10ххх, а остальное под непосредственно "полезную" нагрузку - это 32 - 11 = 21.

Раз в UTF-кодировках для представления одного символа используются разное количество байт, эти кодировки называются кодировками переменной длины. UTF-8, UTF-16, UTF-32 используют разные алгоритмы кодирования кодов таблицы Unicode. Все описанное выше было справедливо для UTF-8.

## Резюме

| Служебные биты | Значение                                              |
| -------------- | ----------------------------------------------------- |
| 0xxxxxxx       | Байт самостоятельный, представляет собой один символ  |
| 10xxxxxx       | Байт является частью "многобайтового" символа         |
| 110xxxxx       | Первый байт символа, представленного двумя байтами    |
| 1110xxxx       | Первый байт символа, представленного тремя байтами    |
| 11110xxx       | Первый байт символа, представленного четырьмя байтами |

Таким образом первый 0 всегда служит этаким маркером, разделяющим служебку и полезную нагрузку.